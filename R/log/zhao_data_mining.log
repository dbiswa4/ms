'data.frame':	150 obs. of  5 variables:
 $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
 $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
 $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
 $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
 $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1 1 1 1 1 1 1 ...
'data.frame':	71 obs. of  10 variables:
 $ age         : num  57 65 59 58 60 61 56 60 58 62 ...
 $ DEXfat      : num  41.7 43.3 35.4 22.8 36.4 ...
 $ waistcirc   : num  100 99.5 96 72 89.5 83.5 81 89 80 79 ...
 $ hipcirc     : num  112 116.5 108.5 96.5 100.5 ...
 $ elbowbreadth: num  7.1 6.5 6.2 6.1 7.1 6.5 6.9 6.2 6.4 7 ...
 $ kneebreadth : num  9.4 8.9 8.9 9.2 10 8.8 8.9 8.5 8.8 8.8 ...
 $ anthro3a    : num  4.42 4.63 4.12 4.03 4.24 3.55 4.14 4.04 3.91 3.66 ...
 $ anthro3b    : num  4.95 5.01 4.74 4.48 4.68 4.06 4.52 4.7 4.32 4.21 ...
 $ anthro3c    : num  4.5 4.48 4.6 3.91 4.15 3.64 4.31 4.47 3.47 3.6 ...
 $ anthro4     : num  6.13 6.37 5.82 5.66 5.91 5.14 5.69 5.7 5.49 5.25 ...
 [1]  1  2  3  4  5  6  7  8  9 10
 [1]  1  2  3  4  5  6  7  8  9 10

    setosa versicolor  virginica 
        50         50         50 
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
4          4.6         3.1          1.5         0.2  setosa
5          5.0         3.6          1.4         0.2  setosa
6          5.4         3.9          1.7         0.4  setosa
[1] 150   5
'data.frame':	150 obs. of  5 variables:
 $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
 $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
 $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
 $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
 $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1 1 1 1 1 1 1 ...
[1] "Sepal.Length" "Sepal.Width"  "Petal.Length" "Petal.Width"  "Species"     
$names
[1] "Sepal.Length" "Sepal.Width"  "Petal.Length" "Petal.Width"  "Species"     

$row.names
  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18
 [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36
 [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54
 [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72
 [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90
 [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108
[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126
[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144
[145] 145 146 147 148 149 150

$class
[1] "data.frame"

  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   
 Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  
 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  
 Median :5.800   Median :3.000   Median :4.350   Median :1.300  
 Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  
 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  
 Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  
       Species  
 setosa    :50  
 versicolor:50  
 virginica :50  
                
                
                
  0%  25%  50%  75% 100% 
 4.3  5.1  5.8  6.4  7.9 
 10%  30%  65% 
4.80 5.27 6.20 
[1] 0.6856935
[1] 1.274315
[1] 0.8717538
             Sepal.Length Sepal.Width Petal.Length Petal.Width
Sepal.Length    0.6856935  -0.0424340    1.2743154   0.5162707
Sepal.Width    -0.0424340   0.1899794   -0.3296564  -0.1216394
Petal.Length    1.2743154  -0.3296564    3.1162779   1.2956094
Petal.Width     0.5162707  -0.1216394    1.2956094   0.5810063
             Sepal.Length Sepal.Width Petal.Length Petal.Width
Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411
Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259
Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654
Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000
     Species Sepal.Length.Min. Sepal.Length.1st Qu. Sepal.Length.Median
1     setosa             4.300                4.800               5.000
2 versicolor             4.900                5.600               5.900
3  virginica             4.900                6.225               6.500
  Sepal.Length.Mean Sepal.Length.3rd Qu. Sepal.Length.Max.
1             5.006                5.200             5.800
2             5.936                6.300             7.000
3             6.588                6.900             7.900




les (or graphics devices) need be closed with graphics.off() or dev.off() after
> #plotting.
>  # save as a PDF file
>    pdf("myPlot.pdf")
>  x <- 1:50
>  plot(x, log(x))
>  graphics.off()
>  #
>    # Save as a postscript file
>    postscript("myPlot2.ps")
>  x <- -20:20
>  plot(x, x^2)
>  graphics.off()
> 
> 
> 
> 
> 
> 
> #CH-4 : Decision Trees and Random Forest
> ########################################
> 
> # packages : party, rpart and randomForest
> 
> ##party example##
> 
> #Note:
> #sample(2, nrow(iris)
> #It will create a vector of 150 elements i.e. nrow(iris) and each element will have value either 1 or 2.
> #The ratio of 1 and 2 wil be 70:30.
> #str(ind)
> #int [1:150] 2 1 1 1 2 2 1 1 1 1 ...
> #
> #If replace=FALSE, then the interpretation of the parameter changes. First parameter indicates
> #total number of observations and 2nd parameter indicates how many observations to take as a sample
> 
> ind <- sample(2, nrow(iris), replace=TRUE, prob=c(0.7, 0.3))
> 
> #Vector from previous step is used to filter out iris rows into training and test dataset.
> trainData <- iris[ind==1,]
> testData <- iris[ind==2,]
> library(party)
> myFormula <- Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
> iris_ctree <- ctree(myFormula, data=trainData)
> # check the prediction
> table(predict(iris_ctree), trainData$Species)
            
             setosa versicolor virginica
  setosa         37          0         0
  versicolor      0         33         2
  virginica       0          0        30
> print(iris_ctree)

	 Conditional inference tree with 3 terminal nodes

Response:  Species 
Inputs:  Sepal.Length, Sepal.Width, Petal.Length, Petal.Width 
Number of observations:  102 

1) Petal.Length <= 1.9; criterion = 1, statistic = 94.907
  2)*  weights = 37 
1) Petal.Length > 1.9
  3) Petal.Width <= 1.7; criterion = 1, statistic = 46.913
    4)*  weights = 35 
  3) Petal.Width > 1.7
    5)*  weights = 30 
> plot(iris_ctree)
> plot(iris_ctree, type="simple")
> 
> #they are shown as \y" in leaf nodes. For example, node 2 is labeled with \n=40, y=(1, 0, 0)", 
> #which means that it contains 40 training instances and all of them belong to the rest class \setosa".
> 
> # predict on test data
> testPred <- predict(iris_ctree, newdata = testData)
> table(testPred, testData$Species)
            
testPred     setosa versicolor virginica
  setosa         13          0         0
  versicolor      0         16         3
  virginica       0          1        15
> 
> 
> #4.2 Decision Trees with Package rpart
> 
> data("bodyfat", package = "TH.data")
> dim(bodyfat)
[1] 71 10
> 
> attributes(bodyfat)
$names
 [1] "age"          "DEXfat"       "waistcirc"    "hipcirc"      "elbowbreadth" "kneebreadth"  "anthro3a"     "anthro3b"    
 [9] "anthro3c"     "anthro4"     

$row.names
 [1] "47"  "48"  "49"  "50"  "51"  "52"  "53"  "54"  "55"  "56"  "57"  "58"  "59"  "60"  "61"  "62"  "63"  "64"  "65"  "66"  "67" 
[22] "68"  "69"  "70"  "71"  "72"  "73"  "74"  "75"  "76"  "77"  "78"  "79"  "80"  "81"  "82"  "83"  "84"  "85"  "86"  "87"  "88" 
[43] "89"  "90"  "91"  "92"  "93"  "94"  "95"  "96"  "97"  "98"  "99"  "100" "101" "102" "103" "104" "105" "106" "107" "108" "109"
[64] "110" "111" "112" "113" "114" "115" "116" "117"

$class
[1] "data.frame"

> #$names
> #$row.names
> 
> set.seed(1234)
> ind <- sample(2, nrow(bodyfat), replace=TRUE, prob=c(0.7, 0.3))
> bodyfat.train <- bodyfat[ind==1,]
> bodyfat.test <- bodyfat[ind==2,]
> # train a decision tree
> library(rpart)
> myFormula <- DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + kneebreadth
> #minsplit : the minimum number of observations that must exist in a node in order for a split to be attempted.
> bodyfat_rpart <- rpart(myFormula, data = bodyfat.train, control = rpart.control(minsplit = 10))
> attributes(bodyfat_rpart)
$names
 [1] "frame"               "where"               "call"                "terms"               "cptable"            
 [6] "method"              "parms"               "control"             "functions"           "numresp"            
[11] "splits"              "variable.importance" "y"                   "ordered"            

$xlevels
named list()

$class
[1] "rpart"

> 
> #$names
> #$xlevels
> #$class
> 
> #cptable : complexity parameter
> #It serves as a penalty term to control tree size and is always monotonic with the number of splits (nsplit). 
> #The smaller the value of CP, the more complex will be the tree (the greater the number of splits).
> #Other parameters of cptable
> #rel error : For a regression tree, the relative error (rel error) is the average deviance of the current 
> #tree divided by the average deviance of the null tree.
> #xerror : The cross-validation error (xerror) is based on a 10-fold cross-validation and is again measured 
> #relative to the deviance of the null model. As expected the cross-validation error is greater than the relative error. 
> #Using the same data to both fit and test a model results in over-optimistic fit diagnostics.
> 
> ##Something to note for further reference
> #http://www.unc.edu/courses/2010spring/ecol/562/001/docs/lectures/lecture22.htm
> #
> #While relative error is guaranteed to decrease as the tree gets more complex, this will not normally be the case for 
> #cross-validation error. Because the cross-validation error is still decreasing in the output shown above, the default 
> #tree size is probably too small. We need to refit the model and force rpart to carry out additional splits. This can be 
> #accomplished with the control argument of rpart. I use the rpart.control function to specify a value for cp= that is 
> #smaller than the default value of 0.01. Because the cross-validation error results are random, I use the set.seed 
> #function first to set the seed for the random number stream so that the results obtained are reproducible.
> #set.seed(20)
> #parrot_tree2 <- rpart(Parrot ~ CoralTotal + as.factor(Month) + as.factor(Station) + as.factor(Method), data = Bahama, control=rpart.control(cp=.001))
> 
> print(bodyfat_rpart$cptable)
          CP nsplit  rel error    xerror       xstd
1 0.67272638      0 1.00000000 1.0194546 0.18724382
2 0.09390665      1 0.32727362 0.4415438 0.10853044
3 0.06037503      2 0.23336696 0.4271241 0.09362895
4 0.03420446      3 0.17299193 0.3842206 0.09030539
5 0.01708278      4 0.13878747 0.3038187 0.07295556
6 0.01695763      5 0.12170469 0.2739808 0.06599642
7 0.01007079      6 0.10474706 0.2693702 0.06613618
8 0.01000000      7 0.09467627 0.2695358 0.06620732
> 
> #Columns you get when rpart model is printed as below
> #n= 56
> #node), split, n, deviance, yval
> #* denotes terminal node
> print(bodyfat_rpart)
n= 56 

node), split, n, deviance, yval
      * denotes terminal node

 1) root 56 7265.0290000 30.94589  
   2) waistcirc< 88.4 31  960.5381000 22.55645  
     4) hipcirc< 96.25 14  222.2648000 18.41143  
       8) age< 60.5 9   66.8809600 16.19222 *
       9) age>=60.5 5   31.2769200 22.40600 *
     5) hipcirc>=96.25 17  299.6470000 25.97000  
      10) waistcirc< 77.75 6   30.7345500 22.32500 *
      11) waistcirc>=77.75 11  145.7148000 27.95818  
        22) hipcirc< 99.5 3    0.2568667 23.74667 *
        23) hipcirc>=99.5 8   72.2933500 29.53750 *
   3) waistcirc>=88.4 25 1417.1140000 41.34880  
     6) waistcirc< 104.75 18  330.5792000 38.09111  
      12) hipcirc< 109.9 9   68.9996200 34.37556 *
      13) hipcirc>=109.9 9   13.0832000 41.80667 *
     7) waistcirc>=104.75 7  404.3004000 49.72571 *
> 
> plot(bodyfat_rpart)
> #Not sure what use.n=T does?
> text(bodyfat_rpart, use.n=T)
> 
> #Then we select the tree with the minimum prediction error
> #In our case, it generated 8 trees. We are selecting the tree with least xerror
> opt <- which.min(bodyfat_rpart$cptable[,"xerror"])
> cp <- bodyfat_rpart$cptable[opt, "CP"]
> cp <- bodyfat_rpart$cptable[8, "CP"]
> bodyfat_prune <- prune(bodyfat_rpart, cp = cp)
> print(bodyfat_prune)
n= 56 

node), split, n, deviance, yval
      * denotes terminal node

 1) root 56 7265.0290000 30.94589  
   2) waistcirc< 88.4 31  960.5381000 22.55645  
     4) hipcirc< 96.25 14  222.2648000 18.41143  
       8) age< 60.5 9   66.8809600 16.19222 *
       9) age>=60.5 5   31.2769200 22.40600 *
     5) hipcirc>=96.25 17  299.6470000 25.97000  
      10) waistcirc< 77.75 6   30.7345500 22.32500 *
      11) waistcirc>=77.75 11  145.7148000 27.95818  
        22) hipcirc< 99.5 3    0.2568667 23.74667 *
        23) hipcirc>=99.5 8   72.2933500 29.53750 *
   3) waistcirc>=88.4 25 1417.1140000 41.34880  
     6) waistcirc< 104.75 18  330.5792000 38.09111  
      12) hipcirc< 109.9 9   68.9996200 34.37556 *
      13) hipcirc>=109.9 9   13.0832000 41.80667 *
     7) waistcirc>=104.75 7  404.3004000 49.72571 *
> 
> plot(bodyfat_prune)
> text(bodyfat_prune, use.n=T)
> 
> 
> #After that, the selected tree is used to make prediction and the predicted values are compared
> #with actual labels. In the code below, function abline() draws a diagonal line. The predictions
> #of a good model are expected to be equal or very close to their actual values, that is, most points
> #should be on or close to the diagonal line.
> 
> DEXfat_pred <- predict(bodyfat_prune, newdata=bodyfat.test)
> xlim <- range(bodyfat$DEXfat)
> #Plotting predicted vs actual value. e.g. For an observation, if predicted value is
> #11, and actual value is also 11, then the point will fall on the diagonal line. If 
> #there is any difference, it will be evident from this plot.
> plot(DEXfat_pred ~ DEXfat, data=bodyfat.test, xlab="Observed", ylab="Predicted", ylim=xlim, xlim=xlim)
> 
> #y=a + bx
> #with a=0, b=1 the line passes thru origin as y becomes x(y=x)
> abline(a=0, b=1)
> 
> 
> 
> #4.3 Random Forest#
> 
> # k time repeat the following procedure
> # - draw bootstrap sample from dataset
> # - train decision tree
> # Until tree is of maximum size
> # Choose next leaf node
> # select m attributes at random
> # 
> # - Measure out-of-bag error
> # -When u drwa bootstrap, u might get duplicate. when duplicate, the othe one may have left out
> # - evaluate the samples that were not selected in bootstrap
> # - provides
> # - measure of strength (inverse error rate)
> # - correlation between trees(which increases the forest error rate)
> # - variable importance
> # 
> # 
> # *** make prediction by majority vote among the k trees
> 
> # Variable Importance
> # 
> # Gini Co-efficeient
> # - measures inequality
> # 
> #
> # Random Forest on Big Data
> # - Easy to parrelelize (trees are build independently)
> # - Handles 'small n big p' problems naturally
> # 
> # 
> # Summary : Decision Trees and Forests
> # 
> # *** Represenation
> # - Decision Trees 
> # - Sets of decision trees with majority vote
> # 
> # *** Evaluation
> # - Accuracy 
> # - Random Forests : out-of-bag error (generated on ur training set)
> # 
> # *** Optimization
> # - Information Gain or Gini Index
> # 
> # Adv.
> # categorical attributes
> # many attributes
> # diff variations of them
> # simple to interpret the result of
> # simple to implement
> # 
> # 
> # Pretty fantastic general purpose
> 
> 
> #There are two limitations with function randomForest()
> #First, it cannot handle data with missing values, and users have to impute data before 
> #feeding them into the function.
> #Second, there is a limit of 32 to the maximum number of levels of each categorical attribute.
> #Attributes with more than 32 levels have to be transformed first before using randomForest().
> 
> #An alternative is cforest() from package party. categorical variables with more levels will take more
> #cpu and memory
> 
> #The iris data is first split into two subsets: training (70%) and test (30%).
> ind <- sample(2, nrow(iris), replace=TRUE, prob=c(0.7, 0.3))
> trainData <- iris[ind==1,]
> testData <- iris[ind==2,]
> 
> #"Species ~ .", which means to predict Species with all other variables in the data.
> 
> library(randomForest)
> rf <- randomForest(Species ~ ., data=trainData, ntree=100, proximity=TRUE)
> table(predict(rf), trainData$Species)
            
             setosa versicolor virginica
  setosa         36          0         0
  versicolor      0         31         2
  virginica       0          1        34
> print(rf)

Call:
 randomForest(formula = Species ~ ., data = trainData, ntree = 100,      proximity = TRUE) 
               Type of random forest: classification
                     Number of trees: 100
No. of variables tried at each split: 2

        OOB estimate of  error rate: 2.88%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         36          0         0  0.00000000
versicolor      0         31         1  0.03125000
virginica       0          2        34  0.05555556
> 
> #Few importnat parameters when model is printed
> #No. of variables tried at each split :
> #OOB estimate of error rate : 
> #Confusion matrix : 
> 
> attributes(rf)
$names
 [1] "call"            "type"            "predicted"       "err.rate"        "confusion"       "votes"           "oob.times"      
 [8] "classes"         "importance"      "importanceSD"    "localImportance" "proximity"       "ntree"           "mtry"           
[15] "forest"          "y"               "test"            "inbag"           "terms"          

$class
[1] "randomForest.formula" "randomForest"        

> 
> #After that, we plot the error rates with various number of trees.
> plot(rf)
> 
> #The importance of variables can be obtained with functions importance() and varImpPlot().
> importance(rf)
             MeanDecreaseGini
Sepal.Length         6.913882
Sepal.Width          1.282567
Petal.Length        26.267151
Petal.Width         34.163836
> varImpPlot(rf)
> 
> #Finally, the built random forest is tested on test data, and the result is checked with functions
> #table() and margin(). The margin of a data point is as the proportion of votes for the correct
> #class minus maximum proportion of votes for other classes. Generally speaking, positive margin means 
> #correct classification.
> irisPred <- predict(rf, newdata=testData)
> table(irisPred, testData$Species)
            
irisPred     setosa versicolor virginica
  setosa         14          0         0
  versicolor      0         17         3
  virginica       0          1        11
> 
> plot(margin(rf, testData$Species))
> 
